##TODO
#Prepare config file
#Adapt rull all

#By Emmanuel Cazottes, adapte from Gaël Castel code
configfile: "config.yaml"

# D165T32, D165T33, D165T34 corrupted files? <- CAUTION about that
WORKDIR = config["WORKDIR"]
ref_fa=config["REFTARGET"]
#INDIR=config["INPATH"]
dbsnp=config["DBSNP"]
line_snp=config["LINESNP"]
bed_exons=config["BEDEXONS"]

rule all:
    input:
        ase=expand(WORKDIR+"/ASEReadCounter/{sample}.ASE.GENEID.tab", sample=config["SAMPLES"])
        #forward=expand(WORKDIR+"/bigwig/{sample}.BPM.forward.bw", sample=config["SAMPLES"]),
        #reverse=expand(WORKDIR+"/bigwig/{sample}.BPM.reverse.bw", sample=config["SAMPLES"])
        #bam=expand(WORKDIR+"/bam/{sample}/Filtered_bams/{sample}_Filtered.bam",sample=config["SAMPLES"]),
        #bai=expand(WORKDIR+"/bam/{sample}/Filtered_bams/{sample}_Filtered.bam.bai",sample=config["SAMPLES"])

rule getSRR:
    output:
        fastq1=WORKDIR+"/fastq/{sample}/{sample}_1.fastq.gz",
        fastq2=WORKDIR+"/fastq/{sample}/{sample}_2.fastq.gz",
    #log:
        #prefetch="logs/prefetch_{sample}.log",
        #fasterqdump="logs/fasterqdump_{sample}.log"
    shell:"""
    prefetch -O fastq/ {wildcards.sample}
    cd fastq/{wildcards.sample}
    fasterq-dump --split-3 {wildcards.sample}
    gzip -c {wildcards.sample}_1.fastq > {output.fastq1}
    gzip -c {wildcards.sample}_2.fastq > {output.fastq2}
    """

rule trimgalore:
    #priority:4
    input:
        fastq1=WORKDIR+"/fastq/{sample}/{sample}_1.fastq.gz",
        fastq2=WORKDIR+"/fastq/{sample}/{sample}_2.fastq.gz"
    params:
        fastq1_trimmed=WORKDIR+"/fastq/trimmed/{sample}_1_val_1.fq.gz",
        fastq2_trimmed=WORKDIR+"/fastq/trimmed/{sample}_2_val_2.fq.gz"
    output:
        fastq1_trimmed_renamed=WORKDIR+"/fastq/trimmed/{sample}.trimmed.R1.fastq.gz",
        fastq2_trimmed_renamed=WORKDIR+"/fastq/trimmed/{sample}.trimmed.R2.fastq.gz"
    threads: 6
    shell:"""
    trim_galore --length 50 --paired -j {threads} -o {WORKDIR}/fastq/trimmed {input.fastq1} {input.fastq2}
    mv {params.fastq1_trimmed} {output.fastq1_trimmed_renamed}
    mv {params.fastq2_trimmed} {output.fastq2_trimmed_renamed}
    """
# # --length 50 : reads < 50 after trimming are discarded
# # fait automatiquement le fastqc sur les fastq trimmés ??
# # conserve les cell barcodes pour le démultiplexage?
# # Paired-end:
# # The *_trimmed.fq.gz are produced as intermediate output (as R1 and R2 are trimmed individually in the first instance).
# # Once the trimming has completed, Trim Galore will launch a round of 'validation' (which is is where the files get the val in their names from),
# # which primarily performs length-cutoff filtering (and a few more optional things I believe).
# # Once the validation is complete, the trimmed files will be deleted, and you are left with only the files N1_1_val_1.fq.gz and N1_2_val_2.fq.gz.
# # Change these names to match format of already trimmed files given by Charbel

###################################FASTQC_TRIMMED#####################################

rule fastqc_trimmed:
    #priority:3
    input:
        fastq_trimmed1=WORKDIR+"/fastq/trimmed/{sample}.trimmed.R1.fastq.gz",
        fastq_trimmed2=WORKDIR+"/fastq/trimmed/{sample}.trimmed.R2.fastq.gz",
    output: WORKDIR+"/fastqc/{sample}_fastqc.html"
    log:
        out="log/FASTQC_{sample}.out",
        err="log/FASTQC_{sample}.err"
    shell:
        "fastqc -o {output} {input.fastq_trimmed1} {input.fastq_trimmed2} 1> {log.out} 2> {log.err}"

###################################Align reads onto maccaca mulatta reference genome#####################################

rule STAR_calJac4:
    input:
        fw=WORKDIR+"/fastq/trimmed/{sample}.trimmed.R1.fastq.gz",
        rv=WORKDIR+"/fastq/trimmed/{sample}.trimmed.R2.fastq.gz"
    output:
        temp(WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.bam")
    threads: 16
    params:
        index_calJac4=config["RHEMAC10STAR"],
        #gtf_calJac4="genomes/calJac4/annotation/Macaca_mulatta.Mmul_10.108.UCSC.XICRNA.20221129.gtf",
        basename=WORKDIR+"/bam/{sample}.calJac4."
    log:
        WORKDIR+"/log/{sample}_star_calJac4.log"
    shell:
        "STAR --readFilesCommand zcat --outFileNamePrefix {params.basename} \
        --runMode alignReads  --runThreadN {threads} --outFilterType BySJout --outSAMtype BAM SortedByCoordinate \
        --outSAMattributes NM \
        --genomeDir {params.index_calJac4} --readFilesIn {input.fw} {input.rv}"    

#--outSAMattributes NM Essential for downstream XenofiltR

rule low_mapq:
    input:
        bam=WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.bam",
    output:
        temp(WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.q10.bam")
    params: 
        qual=10
    #priority: 3
    log:
        WORKDIR+"log/{sample}_calJac4_lowmapq.log"
    shell:
        "(samtools view -q {params.qual} -b {input.bam} > {output}) 2> {log}"

rule samtools_index_calJac4:
    input: WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.q10.bam"
    output: temp(WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.q10.bam.bai")
    log: WORKDIR+"log/{sample}_samtools_index_calJac4.log"
    params:
        "" # optional params string
    #priority: 3
    threads: 6
    shell:
        "samtools index -b -@ {threads} {input} 2> {log}"

rule MarkDuplicates_calJac4:
    input: 
            bam=WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.q10.bam",
            bai=WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.q10.bai"
    output: temp(WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.q10.dedupped.bam")
    params:
            CREATE_INDEX="true",
            VALIDATION_STRINGENCY="SILENT",
            REMOVE_DUPLICATES="true",
            ASSUME_SORTED="true",
            M="output.metrics"
    log:
        WORKDIR+"log/{sample}_calJac4_MarkDuplicates.log"
    #priority: 6
    shell:
        "(picard MarkDuplicates \
            I={input.bam} \
            O={output} \
            CREATE_INDEX={params.CREATE_INDEX} \
            VALIDATION_STRINGENCY={params.VALIDATION_STRINGENCY} \
            REMOVE_DUPLICATES={params.REMOVE_DUPLICATES} \
            ASSUME_SORTED={params.ASSUME_SORTED} \
            M={params.M}) 2> {log}"

rule samtools_index_calJac4_md:
    input: WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.q10.dedupped.bam"
    output: temp(WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.q10.dedupped.bam.bai")
    params:
        "" # optional params string
    #priority: 3
    threads: 6
    log: WORKDIR+"log/{sample}_samtools_index_calJac4_md.log"
    shell:
        "samtools index -b -@ {threads} {input} 2> {log}"

rule stats_calJac4:
    input:
        bam=WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.q10.dedupped.bam",
        bai=WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.q10.dedupped.bam.bai"
    output: 
        WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.q10.dedupped.txt"
    log: WORKDIR+"log/{sample}_filtered_bam_stat_calJac4.err"
    shell:
        "samtools idxstats {input.bam} > {output} 2> {log}"

###################################Align reads onto mus musculus reference genome#####################################

rule STAR_mm10:
    input:
        fw=WORKDIR+"/fastq/trimmed/{sample}.trimmed.R1.fastq.gz",
        rv=WORKDIR+"/fastq/trimmed/{sample}.trimmed.R2.fastq.gz"
    output:
        temp(WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.bam")
    threads: 16
    params:
        index_mm10=config["MM10STAR"],
        #gtf_mm10="genomes/mm10/annotation/Macaca_mulatta.Mmul_10.108.UCSC.XICRNA.20221129.gtf",
        basename=WORKDIR+"/bam/{sample}.mm10."
    log:
        WORKDIR+"/log/{sample}_star_mm10.log"
    shell:
        "STAR --readFilesCommand zcat --outFileNamePrefix {params.basename} \
        --runMode alignReads  --runThreadN {threads} --outFilterType BySJout --outSAMtype BAM SortedByCoordinate \
        --outSAMattributes NM \
        --genomeDir {params.index_mm10} --readFilesIn {input.fw} {input.rv}"    

#--outSAMattributes NM Essential for downstream XenofiltR

rule low_mapq_mm10:
    input:
        bam=WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.bam",
    output:
        temp(WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.q10.bam")
    params: 
        qual=10
    #priority: 3
    log:
        WORKDIR+"log/{sample}_mm10_lowmapq.log"
    shell:
        "(samtools view -q {params.qual} -b {input.bam} > {output}) \
            2> {log}"

rule samtools_index_mm10:
    input: WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.q10.bam"
    output: temp(WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.q10.bam.bai")
    log: WORKDIR+"log/{sample}_samtools_index_mm10.log"
    params:
        "" # optional params string
    #priority: 3
    threads: 6
    shell:
        "samtools index -b -@ {threads} {input} 2> {log}"

rule MarkDuplicates_mm10:
    input: 
            bam=WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.q10.bam",
            bai=WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.q10.bai"
    output: temp(WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.q10.dedupped.bam")
    params:
            CREATE_INDEX="true",
            VALIDATION_STRINGENCY="SILENT",
            REMOVE_DUPLICATES="true",
            ASSUME_SORTED="true",
            M="output.metrics"
    log:
        WORKDIR+"log/{sample}_mm10_MarkDuplicates.log"
    #priority: 6
    shell:
        "(picard MarkDuplicates \
            I={input.bam} \
            O={output} \
            CREATE_INDEX={params.CREATE_INDEX} \
            VALIDATION_STRINGENCY={params.VALIDATION_STRINGENCY} \
            REMOVE_DUPLICATES={params.REMOVE_DUPLICATES} \
            ASSUME_SORTED={params.ASSUME_SORTED} \
            M={params.M}) 2> {log}"

rule samtools_index_mm10_md:
    input: WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.q10.dedupped.bam"
    output: temp(WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.q10.dedupped.bam.bai")
    params:
        "" # optional params string
    #priority: 3
    threads: 6
    log: WORKDIR+"log/{sample}_samtools_index_mm10_md.log"
    shell:
        "samtools index -b -@ {threads} {input} 2> {log}"

rule stats_mm10:
    input:
        bam=WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.q10.dedupped.bam",
        bai=WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.q10.dedupped.bam.bai"
    output: 
        WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.q10.dedupped.txt"
    log: WORKDIR+"log/{sample}_filtered_bam_stat_mm10.err"
    shell:
        "samtools idxstats {input.bam} > {output} 2> {log}"

###################################XenofiltR#####################################

rule Perform_Xenofilter:
    input: 
        mouse_bam=WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.q10.bam",
        target_bam=WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.q10.bam",
        mouse_bam_bai=WORKDIR+"/bam/{sample}.mm10.Aligned.sortedByCoord.out.q10.bam.bai",
        target_bam_bai=WORKDIR+"/bam/{sample}.calJac4.Aligned.sortedByCoord.out.q10.bam.bai"
    output:
        bam=WORKDIR+"/bam/{sample}/Filtered_bams/{sample}_Filtered.bam",
        bai=WORKDIR+"/bam/{sample}/Filtered_bams/{sample}_Filtered.bam.bai"
    params:
       outname="{sample}",
       NM=4
    log:WORKDIR+"log/{sample}_xenofilter.log"
    shell:
        "(Rscript Xeno.R {input.target_bam} {input.mouse_bam} {params.outname} {params.NM}) 2> {log}"

rule stats_bam_Xeno:
    input:
        bam=WORKDIR+"/bam/{sample}/Filtered_bams/{sample}.calJac4.Aligned.sortedByCoord.out.q10.dedupped_Filtered.bam",
        bai=WORKDIR+"/bam/{sample}/Filtered_bams/{sample}.calJac4.Aligned.sortedByCoord.out.q10.dedupped_Filtered.bam.bai"
    output: 
        WORKDIR+"/bam/{sample}/Filtered_bams/{sample}.calJac4.Aligned.sortedByCoord.out.q10.dedupped.xenofiltRed.txt"
    log: WORKDIR+"log/{sample}_filtered_bam_stat.err"
    shell:
        "samtools idxstats {input.bam} > {output} 2> {log}"

rule bigwig_forward:
    input:
        bam=WORKDIR+"/bam/{sample}/Filtered_bams/{sample}_Filtered.bam",
        bai=WORKDIR+"/bam/{sample}/Filtered_bams/{sample}_Filtered.bam.bai"
    output: 
        WORKDIR+"/bigwig/{sample}.BPM.forward.bw"
    threads: 6
    params:
        filterRNAstrand="forward",
        normalizeUsing="BPM",
        binSize=20,
        smoothLength=40,
    log: "logs/{sample}_BPM_forward.log"
    shell:
        "(bamCoverage -b {input.bam} --filterRNAstrand {params.filterRNAstrand} --normalizeUsing {params.normalizeUsing} \
        --binSize {params.binSize} --smoothLength {params.smoothLength} -p {threads} -o {output}) 2> {log}"

rule bigwig_reverse:
    input:
        bam=WORKDIR+"/bam/{sample}/Filtered_bams/{sample}_Filtered.bam",
        bai=WORKDIR+"/bam/{sample}/Filtered_bams/{sample}_Filtered.bam.bai"
    output: 
        WORKDIR+"/bigwig/{sample}.BPM.reverse.bw"
    threads: 6
    params:
        filterRNAstrand="reverse",
        normalizeUsing="BPM",
        binSize=20,
        smoothLength=40,
    log: "logs/{sample}_BPM_reverse.log"
    shell:
        "(bamCoverage -b {input.bam} --filterRNAstrand {params.filterRNAstrand} --normalizeUsing {params.normalizeUsing} \
        --binSize {params.binSize} --smoothLength {params.smoothLength} -p {threads} -o {output}) 2> {log}"

rule scallop: #Not on cluster
    input:
        bam=("map/{sample}_uniq_sorted.bam"),
        bw=("bigwig/{sample}_BPM_reverse.bw")
    output:
        "annotation/{sample}.scallop.gtf"
    log:
        "logs/scallop/{sample}.log"
    params:
        tx_length=200,
        flk_length=10,
        spl_reads=3,
        gap=25,
        cov=1,
        sgl_cov=20
    shell:
        "(scallop -i {input.bam} \
        --min_transcript_length_base {params.tx_length} \
        --min_flank_length {params.flk_length} \
        --min_splice_bundary_hits {params.spl_reads} \
        --min_bundle_gap {params.gap} \
        --min_transcript_coverage {params.cov} \
        --min_single_exon_coverage {params.sgl_cov} \
        -o {output}) 2> {log}"

####GATK pre-processing ################################################
rule MarkDuplicates:
    input: 
        bam=WORKDIR+"/bam/XenoFilteR/{sample}/Filtered_bams/{sample}_Filtered.bam",
        bai=WORKDIR+"/bam/XenoFilteR/{sample}/Filtered_bams/{sample}_Filtered.bam.bai"
    output: 
        bam=temp(WORKDIR+"/GATKpreProcess/{sample}.dedupped.bam"),
        bai=temp(WORKDIR+"/GATKpreProcess/{sample}.dedupped.bai")
    params:
            CREATE_INDEX="true",
            VALIDATION_STRINGENCY="SILENT",
            #REMOVE_DUPLICATES="true",
            #ASSUME_SORTED="true",
            M="output.metrics",
            regex="'null'" #Disable optical duplicate detection because in RNA-Seq many reads are duplicated
    log:
        "logs/{sample}_MarkDuplicates.log"
    shell:
        "(picard MarkDuplicates \
            I={input.bam} \
            O={output.bam} \
            CREATE_INDEX={params.CREATE_INDEX} \
            VALIDATION_STRINGENCY={params.VALIDATION_STRINGENCY} \
            M={params.M} \
            READ_NAME_REGEX={params.regex}) 2> {log}"

            #REMOVE_DUPLICATES={params.REMOVE_DUPLICATES} \
            #ASSUME_SORTED={params.ASSUME_SORTED} \

rule SplitNCigarReads:
    input:
        bam=WORKDIR+"/GATKpreProcess/{sample}.dedupped.bam",
        bai=WORKDIR+"/GATKpreProcess/{sample}.dedupped.bai"
    output:
        bam=WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.bam",
        bai=WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.bai"
    params:
        ref=ref_fa
    log:
        "logs/{sample}_SplitNCigarReads.log"
    shell:
        "(gatk SplitNCigarReads \
            --create-output-bam-index \
            -R {params.ref} \
            -I {input.bam} \
            -O {output.bam}) 2> {log}"

rule AddOrReplaceReadGroups:
    input:
        bam=WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.bam",
        bai=WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.bai"
    output: 
        bam=temp(WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.rg.added.bam"),
        bai=temp(WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.rg.added.bai")
    params:
        SO="coordinate",
        RGID="rhes.RAPSeq",
        RGLB="library",
        RGPL="platform",
        RGPU="machine",
        RGSM="RG:Z:rhes.RAPSeq",
        INDEX="true"
    #priority: 5
    log:
        "logs/{sample}.rhes.RAPSeq_AddOrReplaceReadGroups.log"
    shell:
        "(picard  AddOrReplaceReadGroups \
            I={input.bam} \
            O={output.bam} \
            SO={params.SO} \
            RGID={params.RGID} \
            RGLB={params.RGLB} \
            RGPL={params.RGPL} \
            RGPU={params.RGPU} \
            RGSM={params.RGSM} \
            CREATE_INDEX={params.INDEX}) 2> {log}"

rule BaseRecalibrator:
    input:
        bam=WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.rg.added.bam",
        #csi=WORKDIR+"/GATKpreProcess/{sample}.dedupped.rg.added.bam.csi",
        bai=WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.rg.added.bai"
    output: 
        WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.rg.added.recal.table"
    params:
        ref=ref_fa,
        dbsnp=dbsnp
    log:
        "logs/{sample}.rhes.RAPSeq_BaseRecalibrator.log"
    shell:
        "(gatk BaseRecalibrator -I {input.bam} \
            -R {params.ref} \
            --known-sites {params.dbsnp} \
            -O {output}) 2> {log}"

# BaseRecalibrator requires .tbi file for ref SNP/indels
# samtools bgzip
#bcftools index -t vcf.gz

rule ApplyBQSR:
    input:
        bam=WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.rg.added.bam",
        bqsr=WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.rg.added.recal.table",
        #csi=WORKDIR+"/GATKpreProcess/{sample}.dedupped.rg.added.bam.csi",
        bai=WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.rg.added.bai"
    output:
        bam=WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.rg.added.recal.bam",
        bai=WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.rg.added.recal.bai"
    params:
        ref=ref_fa,
    log:
        "logs/{sample}.rhes.RAPSeq_ApplyBQSR.log"
    shell:
        "(gatk ApplyBQSR \
            -R {params.ref} \
            -I {input.bam} \
            -bqsr {input.bqsr} \
            --create-output-bam-index \
            -O {output.bam}) 2> {log}"

rule ASEReadCounter:
    input:
        bam=WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.rg.added.recal.bam",
        vcf=line_snp,
        bai=WORKDIR+"/GATKpreProcess/{sample}.dedupped.split.rg.added.recal.bai"
    output:
        WORKDIR+"/ASEReadCounter/{sample}.ASE.tab"
    params:
        ref=ref_fa
    log:
        "logs/{sample}.ase.log"
    shell:
        "gatk ASEReadCounter -I {input.bam} -V {input.vcf} -O {output} -R {params.ref} 2> {log}"

rule aseAnnotated:
    input:
        WORKDIR+"/ASEReadCounter/{sample}.ASE.tab"
    output:
        WORKDIR+"/ASEReadCounter/{sample}.ASE.GENEID.tab"
    params:
        bed=bed_exons
    log:
        "logs/{sample}.ase.geneid.log"
    shell:"""
    (tail -n +2 {input} | awk -v OFS="\\t" "{{print $1,$2-1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13}}" - | \
    bedtools intersect -a - -b {params.bed} -wo | cut -f 1-14,18 - > {output}) 2> {log}
    """